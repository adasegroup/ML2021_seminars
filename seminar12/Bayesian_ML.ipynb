{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adasegroup/ML2021_seminars/blob/master/seminar12/Bayesian_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 12: Bayesian Neural Netwroks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a dataset of a small size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_plot(N=100, noise=0., random_state=None, train_size=0.1):\n",
    "    X, y = make_moons(noise=noise, random_state=random_state, n_samples=int(N/train_size))\n",
    "    X, y = Variable(torch.from_numpy(X.astype('float32'))), Variable(torch.from_numpy(y))\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    train_idx, test_idx = train_test_split(range(len(X)), train_size=train_size, random_state=0, stratify=y.cpu())\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "    ax[0].scatter(X[:,0].cpu(), X[:,1].cpu(), c=y.cpu(), cmap='bwr')\n",
    "    ax[0].set(title=f\"Moons of size {10*N}\", xlabel=\"X\", ylabel=\"Y\");\n",
    "\n",
    "    ax[1].scatter(X_train[:,0].cpu(), X_train[:,1].cpu(), c=y_train.cpu(), cmap='bwr')\n",
    "    ax[1].set(title=f\"Train sample of size {N}\", xlabel=\"X\", ylabel=\"Y\");\n",
    "    plt.show()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_and_plot(N=100, noise=0.35, random_state=42, train_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to build a classification neural network on the small train sample. <br>\n",
    "We can see that the data is noisy, and there is no perfect separation between classes. <br>\n",
    "\n",
    "It is **unclear** what decision boundary will be the optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an important moment that we can use <ins>**only**</ins> the train set. <br>\n",
    "We will test our model, but we can't choose the number of epochs or other hyperparameters looking at the test score. <br>\n",
    "Splitting the train set is also a bad idea since there are too few observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a simple Fully-Connected NN with three layers, batch-normalization, and tanh activation. <br>\n",
    "The first two layers are feature-extractors. The last layer is the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    Linear = nn.Linear\n",
    "    \n",
    "    def __init__(self, in_features=1, hidden=1, n_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            self.Linear(in_features, hidden, bias=False),\n",
    "            nn.BatchNorm1d(hidden), nn.Tanh(),\n",
    "            self.Linear(hidden, hidden, bias=False),\n",
    "            nn.BatchNorm1d(hidden), nn.Tanh()\n",
    "        )\n",
    "        self.out = nn.Linear(hidden, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.log_softmax(self.out(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the plotting function. It is just a hand-written tool for plotting the boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundaries(model, X, y, X_test, y_test, epoch, plot_uncertainty=False, print_score=False, alpha=1.0,\n",
    "                   fig = None, ax=None, samples=1):\n",
    "    model.eval()\n",
    "    \n",
    "    xx = np.linspace(X.cpu()[:,0].min()-1., X.cpu()[:,0].max()+1., 50)\n",
    "    yy = np.linspace(X.cpu()[:,1].min()-1., X.cpu()[:,1].max()+1., 50)\n",
    "    mesh = np.meshgrid(xx, yy)\n",
    "    a = np.zeros((2500,2))\n",
    "    a[:,0], a[:,1] = np.ravel(mesh[0]), np.ravel(mesh[1])\n",
    "    contour_test = torch.Tensor(a).to(device)\n",
    "\n",
    "    if 'ensemble_pred' in globals():\n",
    "        if plot_uncertainty:\n",
    "            cmap = sns.cubehelix_palette(light=1, as_cmap=True)\n",
    "            _, predict_out = ensemble_pred(model, contour_test, N=1000)\n",
    "        else:\n",
    "            cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\n",
    "            predict_out, _ = ensemble_pred(model, contour_test, N=100)\n",
    "    else:\n",
    "        cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\n",
    "        predict_out = torch.exp(model(contour_test))\n",
    "\n",
    "    contour_plot = predict_out.cpu().detach().numpy()[:,1]\n",
    "   \n",
    "    title = f'epoch: {epoch} | '\n",
    "    if print_score:\n",
    "        if 'ensemble_pred' in globals(): \n",
    "            y_pred, _ = ensemble_pred(model, X, N=samples)\n",
    "        else:\n",
    "            y_pred = model(X)\n",
    "        train_score = (y_pred.argmax(1)==y).float().mean().item()\n",
    "        title += f'train: {100*train_score:.2f}% | ' \n",
    "        \n",
    "        if 'ensemble_pred' in globals(): \n",
    "            y_pred, _ = ensemble_pred(model, X_test, N=samples)\n",
    "        else:\n",
    "            y_pred = model(X_test)\n",
    "        test_score = (y_pred.argmax(1)==y_test).float().mean().item()\n",
    "        title += f'test: {100*test_score:.2f}%' \n",
    "\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    contour = ax.contourf(mesh[0], mesh[1], contour_plot.reshape(50,50), 10, cmap=cmap);\n",
    "    #cbar = plt.colorbar(contour)\n",
    "    ax.scatter(X.cpu()[:,0], X.cpu()[:,1], c=y.cpu(), cmap='bwr', alpha=alpha)\n",
    "    sns.despine()\n",
    "    ax.set(xlabel='X', ylabel='y', title=title)\n",
    "    cbar = plt.colorbar(contour, ax=ax)\n",
    "    if fig is None:\n",
    "        plt.show()\n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Write a training procedure for NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, criterion, optimizer):\n",
    "    '''\n",
    "    model - our NN\n",
    "    X - input Tensor with size (batch_size, n_features)\n",
    "    y - labels Tensor with size (batch_size)\n",
    "    criterion - loss function\n",
    "    optimizer - optimizer\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look on the visualization of training progress. <br>\n",
    "We use Adam optimizer. <br>\n",
    "Our loss function is Negative Log-Likelihood Loss (NLLLoss).<br>\n",
    "\n",
    "We will fix weight initialization for better comparison later.\n",
    "\n",
    "We have a toy and a low-memory dataset, so there is no need to uses batches. We will use the whole set for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model = Net(in_features=2, hidden=5, n_classes=2).to(device)\n",
    "\n",
    "epochs = 5000\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss(reduction='sum')\n",
    "\n",
    "train_score, test_score = plot_boundaries(model, X_train, y_train, X_test, y_test, 0,\n",
    "                                          alpha=0.5, plot_uncertainty=False, print_score=True)\n",
    "plt.show()\n",
    "clear_output(wait=True)\n",
    "\n",
    "train_scores, test_scores = [train_score], [test_score]\n",
    "\n",
    "steps = (np.logspace(-np.log10(epochs), 0, 200)*epochs).astype(int)\n",
    "steps = np.unique(np.hstack([0, steps]))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, X_train, y_train, criterion, optimizer)\n",
    "        \n",
    "    if epoch in steps:        \n",
    "        train_score, test_score = plot_boundaries(model, X_train, y_train, X_test, y_test, epoch,\n",
    "                                                  alpha=0.5, plot_uncertainty=False, print_score=True)\n",
    "        plt.show()\n",
    "        clear_output(wait=True) \n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen quite strange behavior outside our set. It is because we consider only training set and other areas of feature space out of our interest. <br>\n",
    "Also, we see that the model starts to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the score function during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(steps, train_scores, label='train')\n",
    "plt.plot(steps, test_scores, label='test')\n",
    "plt.legend()\n",
    "plt.title('Accuracy of Simple NN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that score starts to degrade, which means that the model starts to overfit. \n",
    "\n",
    "There were iterations where when our model didn't start to overfit, but we can't catch this moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian approach: Bayesian Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Rule:\n",
    "\n",
    "$$\n",
    "p(A|B) = \\frac{p(B|A)p(A)}{p(B)},\n",
    "$$\n",
    "\n",
    "$p(A)$ - prior probability,<br>\n",
    "$p(A|B)$ - posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the data $D$. We assume that the weight $W$ of our model has prior distribution $p(W)$. Then we get\n",
    "\n",
    "$$\n",
    "p(W|D) = \\frac{p(D|W)p(W)}{p(D)} =\\frac{p(D|W)p(W)}{\\int p(D|W)p(W)dW}.\n",
    "$$\n",
    "The $p(D|W)$ is the likelihood, and $p(W|D)$ is the posterior distribution of our weights.<br>\n",
    "The denominator, in most cases, is intractable, so we can't compute the posterior explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to approximate the true posterior with some parametric distribution $q_{\\theta}(W)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://github.com/adasegroup/ML2021_seminars/raw/main/seminar12/imgs/posterior_approx.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our task now is to minimize KL-divergence between these distributions.<br>\n",
    "But we don't know the true posterior!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "KL\\big(q_{\\theta}(W)\\|p(W|D)\\big) \\to \\underset{\\theta}{min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite KL-divergence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "KL\\big(q_{\\theta}(W)\\|p(W|D)\\big) = \\int q_{\\theta}(W)\\ln\\frac{q_{\\theta}(W)p(D)}{p(W,D)}dW = \\int q_{\\theta}(W)\\bigg[\\ln\\frac{q_{\\theta}(W)}{p(D|W)p(W)} + \\ln p(D)\\bigg] dW;\n",
    "$$\n",
    "\n",
    "$$\n",
    "KL\\big(q_{\\theta}(W)\\|p(W|D)\\big) = \\int q_{\\theta}(W)\\ln\\frac{q_{\\theta}(W)}{p(W)}dW - \\int q_{\\theta}(W)\\ln p(D|W)dW + \\ln p(D);\n",
    "$$\n",
    "\n",
    "$$\n",
    "KL\\big(q_{\\theta}(W)\\|p(W|D)\\big) = KL\\big(q_{\\theta}(W)\\|p(W)\\big) - E_{q_{\\theta}(W)}\\ln p(D|W) + \\ln p(D);\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ln p(D) - KL\\big(q_{\\theta}(W)\\|p(W|D)\\big) = E_{q_{\\theta}(W)}\\ln p(D|W) - KL\\big(q_{\\theta}(W)\\|p(W)\\big).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidence Lower Bound (ELBO):\n",
    "$$\n",
    "ELBO(X) = E_{q_{\\theta}(W)}\\ln p(D|W) - KL\\big(q_{\\theta}(W)\\|p(W)\\big) \\to \\underset{\\theta}{max}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $KL\\big(q_{\\theta}(W)\\|p(W|D)\\big)\\geqslant 0$, this equation shows that the evidence lower bound is indeed a lower bound on the log-evidence $\\ln P(D)$ for the model considered. As $\\ln P(D)$ does not depend on $\\theta$ this equation additionally shows that maximizing the evidence lower bound on the right minimizes $KL\\big(q_{\\theta}(W)\\|p(W|D)\\big)$, as claimed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case the, $-E_{q_{\\theta}(W)}\\ln p(D|W)$ is the negative log-likelihood loss (NLLLoss) (**not averaged!**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we want to maximize ELBO with Gradient Descend and update weights with backpropagation.<br>\n",
    "How to backpropagate through random weight?<br>\n",
    "\n",
    "Answer: Reparameterization trick [(Kingma, 2014)](https://arxiv.org/abs/1312.6114)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://github.com/adasegroup/ML2021_seminars/raw/main/seminar12/imgs/trick.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. \n",
    "Suppose that all weights are independent.<br>\n",
    "Let the prior be a factorized $N(0,I)$ and we approximate true posterior with factorized $N(a_i, \\sigma_i^2I)$, i.e., factorized normal distribution with mean vector $a$ and covariance matrix $\\sigma^2I$.<br>\n",
    "Write a function for the reparametrization trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnormal(mean, var):\n",
    "    '''\n",
    "    mean - mean value\n",
    "    var - variance\n",
    "    \n",
    "    output: \n",
    "    a sample from N(mean, var) and the same shape as mean.\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, maximizing ELBO is maximizing log-likelihood with additional weights regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we also suppose that weights are independent.\n",
    "We use factorized $N(0,I)$ prior and we approximate true posterior with factorized $N(a_i, \\sigma_i^2I)$.<br>\n",
    "\n",
    "It means that each weight's distribution is defined by its mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a BayesLinear class for Bayesian fully-connected layer. <br>\n",
    "**Question**: How to inizialize weight's variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: with small values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.\n",
    "Write the **.forward** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesLinear(nn.Module):\n",
    "    def __init__(self, in_features=1, out_features=1, bias=True):\n",
    "        super(BayesLinear, self).__init__()\n",
    "\n",
    "        self.add_bias = bias\n",
    "        \n",
    "        # W_mean\n",
    "        self.weight = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "        nn.init.xavier_normal_(self.weight)\n",
    "\n",
    "        # W_variance\n",
    "        self.weight_var = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "        nn.init.constant_(self.weight_var, -10.)\n",
    "        \n",
    "        if self.add_bias:\n",
    "            # b_mean\n",
    "            self.bias = nn.Parameter(torch.zeros(1, out_features))\n",
    "            nn.init.xavier_normal_(self.bias, 0)\n",
    "            \n",
    "            # b_variance\n",
    "            self.bias_var = nn.Parameter(torch.zeros(1, out_features))\n",
    "            nn.init.constant_(self.bias_var, -10.)\n",
    "    \n",
    "    def kl(self, prior):\n",
    "        p = torch.distributions.Normal(self.weight, torch.sqrt(F.softplus(self.weight_var)))\n",
    "        kl = torch.distributions.kl_divergence(p, prior).sum()\n",
    "        if self.add_bias:\n",
    "            p = torch.distributions.Normal(self.bias, torch.sqrt(F.softplus(self.bias_var)))\n",
    "            kl += torch.distributions.kl_divergence(p, prior).sum()\n",
    "        return kl\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x - input Tensor of size (batch_size, in_features)\n",
    "        '''\n",
    "        # Sample W, b given its mean and variance. Then make linear prediction\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change our feature extractor's linear layers with bayesian ones.<br>\n",
    "Also, we add a function to compute KL-divergence over all BaysianLinear layers in our net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(Net):\n",
    "    Linear = BayesLinear\n",
    "    \n",
    "    def kl(self, prior):\n",
    "        kl = 0\n",
    "        for x in self.modules():\n",
    "            if isinstance(x, BayesLinear):\n",
    "                kl += x.kl(prior)\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of Bayesian NN are not deterministic. The weights are sampled each time.<br>\n",
    "To more accurate results, you should average the predicted probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://github.com/adasegroup/ML2021_seminars/raw/main/seminar12/imgs/dist.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.\n",
    "Write the **ensemble_pred** function, which returns the mean and standard deviation of N sampled **probabilities**.<br>\n",
    "Please take into account that model outputs are **log-probabilities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_pred(model, X, N=1):\n",
    "    '''\n",
    "    model - our NN\n",
    "    X - input tensor of size (batch_size, n_features)\n",
    "    N - number of samples\n",
    "    \n",
    "    output: (mean, std) - mean and std of N probabilities for each observation.\n",
    "    '''\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again our classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_and_plot(N=100, noise=0.35, random_state=42, train_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the util function to fix weights initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initBayesNet(model, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    model_buf = Net(in_features=2, hidden=5, n_classes=2).to(device)\n",
    "    params1 = model.named_parameters()\n",
    "    params2 = model_buf.named_parameters()\n",
    "    dict_params1 = dict(params1)\n",
    "\n",
    "    for name2, param2 in params2:\n",
    "        if name2 in dict_params1:\n",
    "            dict_params1[name2].data.copy_(param2.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesNet(in_features=2, hidden=5, n_classes=2).to(device) \n",
    "initBayesNet(model, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the prior for our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = torch.distributions.Normal(torch.Tensor([0.]).to(device), torch.Tensor([1.]).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.\n",
    "Write a training procedure for Bayesian NN.<br>\n",
    "*Hint*: there will be the only difference in the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bayes(model, X, y, criterion, optimizer):\n",
    "    '''\n",
    "    model - our NN\n",
    "    X - input Tensor with size (batch_size, n_features)\n",
    "    y - labels Tensor with size (batch_size)\n",
    "    criterion - Negative Log-Likelihood\n",
    "    optimizer - optimizer\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss(reduction='sum')\n",
    "\n",
    "train_score, test_score = plot_boundaries(model, X_train, y_train, X_test, y_test, 0,\n",
    "                                          alpha=0.5, plot_uncertainty=False, print_score=True, samples=100)\n",
    "plt.show()\n",
    "clear_output(wait=True)\n",
    "\n",
    "train_scores, test_scores = [train_score], [test_score]\n",
    "\n",
    "steps = (np.logspace(-np.log10(epochs), 0, 200)*epochs).astype(int)\n",
    "steps = np.unique(np.hstack([0, steps]))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train_bayes(model, X_train, y_train, criterion, optimizer)   \n",
    "        \n",
    "    if epoch in steps:        \n",
    "        train_score, test_score = plot_boundaries(model, X_train, y_train, X_test, y_test, epoch,\n",
    "                                                  alpha=0.5, plot_uncertainty=False, print_score=True, samples=100)\n",
    "        plt.show()\n",
    "        clear_output(wait=True) \n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision boundary becomes smoother and wider.<br>\n",
    "The regularization works!\n",
    "\n",
    "Let's plot the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(steps, train_scores, label='train')\n",
    "plt.plot(steps, test_scores, label='test')\n",
    "plt.legend()\n",
    "plt.title('Accuracy of Bayesian NN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No overfitting!!!<br>\n",
    "As we can see bayesian approach helps us to regularize weights. <br>\n",
    "The model doesn't start to overfit during the entire training process. So we can be calm and get a good classifier at the end of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, the prediction of the bayesian model is not fixed. Such a phenomenon is called uncertainty.<br>\n",
    "Our model is uncertain about prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is our model uncertain everywhere, or are there specific areas where it so? Let's look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundaries(model, X_train, y_train, X_test, y_test, epoch,\n",
    "                        alpha=0.5, plot_uncertainty=True, print_score=True, samples=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, an uncertain area is a boundary. <br>\n",
    "The most uncertain area is the edges of the boundary. It is reasonable since we don't actually know how the moons will behave there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison on not-small data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in ELBO loss, the more data we have, the less KL-divergence affects the final loss.<br>\n",
    "Consider the dataset of a bigger size. Will we save the profits of Bayesian NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_and_plot(N=5000, noise=0.35, random_state=42, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model_1 = Net(in_features=2, hidden=5, n_classes=2).to(device)\n",
    "\n",
    "model_2 = BayesNet(in_features=2, hidden=5, n_classes=2).to(device)\n",
    "initBayesNet(model_2, seed=42)\n",
    "\n",
    "epochs = 5000\n",
    "lr = 0.001\n",
    "optimizer_1 = torch.optim.Adam(model_1.parameters(), lr=lr)\n",
    "optimizer_2 = torch.optim.Adam(model_2.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='sum')\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "fig.suptitle('Simple NN vs Bayesian NN')\n",
    "             \n",
    "plot_boundaries(model_1, X_train, y_train, X_test, y_test, 0,\n",
    "                alpha=0.01, plot_uncertainty=False, print_score=True, fig=fig, ax=ax[0])\n",
    "plot_boundaries(model_2, X_train, y_train, X_test, y_test, 0,\n",
    "                alpha=0.01, plot_uncertainty=False, print_score=True, fig=fig, ax=ax[1], samples=10)\n",
    "plt.show()\n",
    "clear_output(wait=True)\n",
    "\n",
    "steps = (np.logspace(-np.log10(epochs), 0, 200)*epochs).astype(int)\n",
    "steps = np.unique(np.hstack([0, steps]))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train(model_1, X_train, y_train, criterion, optimizer_1)\n",
    "    \n",
    "    train_bayes(model_2, X_train, y_train, criterion, optimizer_2) \n",
    "        \n",
    "    if epoch in steps:\n",
    "        fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "        fig.suptitle('Simple NN vs Bayesian NN')\n",
    "        plot_boundaries(model_1, X_train, y_train, X_test, y_test, epoch,\n",
    "                        alpha=0.01, plot_uncertainty=False, print_score=True, fig=fig, ax=ax[0])\n",
    "        plot_boundaries(model_2, X_train, y_train, X_test, y_test, epoch,\n",
    "                        alpha=0.01, plot_uncertainty=False, print_score=True, fig=fig, ax=ax[1], samples=10)\n",
    "        plt.show()\n",
    "        clear_output(wait=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a larger amount of data, Simple NN became more stable and less prone to overfitting.<br>\n",
    "The scores are quite similar. But we can see that the boundary of Bayesian NN is still more generalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
