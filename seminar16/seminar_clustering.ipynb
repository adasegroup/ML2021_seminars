{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adasegroup/ML2021_seminars/blob/master/seminar16/seminar_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar: Clustering\n",
    "\n",
    "#### Seminar structure:\n",
    "\n",
    "* Clustering for IRIS data\n",
    "* Scoring for cluster analysis\n",
    "* Clustering methods\n",
    "* Recap: dimensionality reduction, anomaly detection and clustering\n",
    "\n",
    "#### Seminar interactive board: https://www.menti.com/7mxafef729"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cluster analysis, what for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print('Nice and very accurate classification, and why do we need cluster analysis?', np.round(clf.score(X_test, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If conventional classification methods solve the problem with high sores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot\n",
    "import plotly.graph_objs as go\n",
    "import colorlover as cl\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "def plot_3d (X, y, title = 'MNIST visualization PCA'):\n",
    "    \n",
    "    digits_3d = pd.DataFrame({\n",
    "        'x': X[:, 0], \n",
    "        'y': X[:, 1], \n",
    "        'z': X[:, 2],\n",
    "        'label': y,\n",
    "    })\n",
    "\n",
    "    colors = cl.scales['10']['qual']['Paired']\n",
    "    data = []\n",
    "\n",
    "    for i in range(10):\n",
    "        x = digits_3d[digits_3d['label'].astype('int') == i]['x']\n",
    "        y = digits_3d[digits_3d['label'].astype('int') == i]['y']\n",
    "        z = digits_3d[digits_3d['label'].astype('int') == i]['z']\n",
    "\n",
    "        color = colors[i]\n",
    "\n",
    "        trace = {\n",
    "            'name': str(i),\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'z': z,\n",
    "            'type': 'scatter3d',\n",
    "            'mode': 'markers',\n",
    "            'marker': {\n",
    "                'size': 5,\n",
    "                'color': color \n",
    "            }\n",
    "        }\n",
    "        data.append(trace)\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title= title,\n",
    "        width=900,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    iplot(fig, show_link = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(X, y, 'IRIS true labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But what if we have no targets at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(X, ### YOUR CODE HERE ###, 'IRIS no labels :(')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. We do not know clustering algorithms, what we can do right now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- apply dimensionality reduction methods;\n",
    "- apply anomaly detection methods;\n",
    "- estimate visually;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3) # just becouse it of 3D visualisation\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "plot_3d(X, np.zeros_like(y), 'PCA() IRIS no labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Scoring methods in cluster analysis:\n",
    "\n",
    "How can we estimate the accurasy of the clustering if we have no labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_labels = np.zeros_like(y)\n",
    "\n",
    "### YOUR CODE HERE ### # defining two classes\n",
    "\n",
    "# let' have look how it worked out\n",
    "plot_3d(X, dummy_labels, 'PCA() IRIS, dummy labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Homogeneity, completeness, and V-measure\n",
    "\n",
    "Homogeneity, completeness, and V-measure are three key related indicators of the quality of a clustering operation. \n",
    "\n",
    "**Homogenity:** scores that each cluster has data-points belonging to the same class label. Homogeneity describes the closeness of the clustering algorithm to this perfection.\n",
    "\n",
    "For $N$ data samples, C different class labels, $K$ clusters and $a_{ck}$ number of data-points belonging to the class c and cluster k. Then the homogeneity $h$ is given by the following:\n",
    "$$h = 1- \\frac{H(C,K)}{H(C)}$$,\n",
    "\n",
    "where \n",
    "$$H(C,K) = - \\sum_{k=1}^{K}\\sum_{c=1}^{C} \\frac{a_{ck}}{N} log(\\frac{a_{ck}}{\\sum_{c=1}^{C}a_{ck}})$$\n",
    "and\n",
    "\n",
    "$$H(C) = -\\sum_{c=1}^{C} \\frac{\\sum_{k=1}^{K} a_{ck}}{C} log(\\frac{\\sum_{k=1}^{K} a_{ck}}{C})$$\n",
    "\n",
    "**Completeness:** scores if all data-points belonging to the same class are clustered into the same cluster. Completeness describes the closeness of the clustering algorithm to this perfection.\n",
    "\n",
    "$$ c = 1 - \\frac{H(K,C)}{H(K)}$$\n",
    "\n",
    "**V-Measure**:\n",
    "\n",
    "`v = (1 + beta) * homogeneity * completeness / (beta * homogeneity + completeness)`\n",
    "\n",
    "\n",
    "Credit for: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Silhouette coefficient\n",
    "\n",
    "The silhouette coefficient contrasts the average distance to elements in the same cluster with the average distance to elements in other clusters. Objects with a high silhouette value are considered well clustered, objects with a low value may be outliers. This index works well with k-means clustering, and is also used to determine the optimal number of clusters.\n",
    "\n",
    "Assume the data have been clustered via any technique, such as `k-means`, into $k$ clusters. \n",
    "\n",
    "For data point $i \\in C_i$ (data point $i$ in the cluster $C_i$), let \n",
    "\n",
    "$$ a(i) = \\frac{1}{|C_i| - 1} \\sum_{j \\in C_i, i \\neq j} d(i, j) $$\n",
    "\n",
    "be the mean distance between $i$ and all other data points in the same cluster, where $d(i, j)$ is the distance between data points $i$ and $j$ in the cluster $C_i$ (we divide by $|C_i| - 1$ because we do not include the distance $d(i, i)$ in the sum). We can interpret $a(i)$ as a measure of how well $i$ is assigned to its cluster (the smaller the value, the better the assignment). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Rand index adjusted for chance.\n",
    "\n",
    "The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4. Mutual Information between two clusterings.\n",
    "Mutual Information between two clusterings.\n",
    "\n",
    "The Mutual Information is a measure of the similarity between two labels of the same data. Where \n",
    " is the number of the samples in cluster \n",
    " and \n",
    " is the number of the samples in cluster \n",
    ", the Mutual Information between clusterings  and  is given as:\n",
    "\n",
    "$$MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\n",
    "\\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}$$\n",
    "\n",
    "\n",
    "This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values wonâ€™t change the score value in any way.\n",
    "\n",
    "Credit for: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html#sklearn.metrics.mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def cluster_scoring(X, y, dummy_labels):\n",
    "    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, dummy_labels))\n",
    "    print(\"Completeness: %0.3f\" % metrics.completeness_score(y, dummy_labels))\n",
    "    print(\"V-measure: %0.3f\" % metrics.v_measure_score(y, dummy_labels))\n",
    "    print(\"Silhouette Score: %.3f\" % metrics.silhouette_score(X, dummy_labels))\n",
    "    print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(y, dummy_labels))\n",
    "    print(\"Mutual Information: %.3f\" % metrics.mutual_info_score(y, dummy_labels))\n",
    "    \n",
    "cluster_scoring(X, y, dummy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And let's define the pivot table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_cluster = pd.DataFrame()\n",
    "\n",
    "def cluster_scoreing_pivot(X, y, dummy_labels, iris_cluster, alg):\n",
    "    cluster_scoring(X, y, dummy_labels)\n",
    "    iris_cluster = iris_cluster.append({'algorithm':alg,\n",
    "                        'hom': metrics.homogeneity_score(y, dummy_labels),\n",
    "                        'comp': metrics.completeness_score(y, dummy_labels),\n",
    "                        'v-measure': metrics.v_measure_score(y, dummy_labels),\n",
    "                        'silhouette': metrics.silhouette_score(X, dummy_labels),\n",
    "                        'rand. index': metrics.adjusted_rand_score(y, dummy_labels),\n",
    "                        'mutual info': metrics.mutual_info_score(y, dummy_labels)}, ignore_index =True) \n",
    "    return iris_cluster \n",
    "\n",
    "iris_cluster = cluster_scoreing_pivot(X, y, dummy_labels, iris_cluster, 'dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering types:\n",
    "\n",
    "__Clustering__ - is unsupervised learning methods for gouping samples.\n",
    "\n",
    "types: \n",
    "\n",
    "    1.1. Centroid-based Clustering\n",
    "    1.2. Density-based Clustering \n",
    "    1.3. Hierarchical Clustering\n",
    "    1.4. Distribution-based Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Centroid-based Clustering: `KMeans`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizes the data into non-hierarchical clusters, in contrast to hierarchical clustering defined below. k-means is the most widely-used centroid-based clustering algorithm. Centroid-based algorithms are efficient but sensitive to initial conditions and outliers. This course focuses on k-means because it is an efficient, effective, and simple clustering algorithm.\n",
    "\n",
    "Credit for: https://developers.google.com/machine-learning/clustering/clustering-algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans # what is the difference between these two?\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "km = ### YOUR CODE HERE ###\n",
    "km.fit(X)\n",
    "\n",
    "plot_3d(X, km.labels_, 'KMeans, n_components = 2')\n",
    "iris_cluster = cluster_scoreing_pivot(X, y, km.labels_, iris_cluster, 'KMeans, 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is we do not know the `n_components`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's go to Seminar #14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will estimate the Intrinsic Dimentionality of IRIS with MLE etimation.\n",
    "\n",
    "NIPS 2004: https://papers.nips.cc/paper/2577-maximum-likelihood-estimation-of-intrinsic-dimension.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    " \n",
    "def intrinsic_dim_sample_wise(X, k=5):\n",
    "    neighb = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
    "    dist, ind = neighb.kneighbors(X) # distances between the samples and points\n",
    "    dist = dist[:, 1:] # the distance between the first points to first points (as basis ) equals zero\n",
    "    # the first non trivial point\n",
    "    dist = dist[:, 0:k]# including points k-1\n",
    "    assert dist.shape == (X.shape[0], k) # requirments are there is no equal points\n",
    "    assert np.all(dist > 0)\n",
    "    d = np.log(dist[:, k - 1: k] / dist[:, 0:k-1]) # dinstance betveen the bayeasan statistics\n",
    "    d = d.sum(axis=1) / (k - 2)\n",
    "    d = 1. / d\n",
    "    intdim_sample = d\n",
    "    return intdim_sample\n",
    " \n",
    "def intrinsic_dim_scale_interval(X, k1=10, k2=20):\n",
    "    X = pd.DataFrame(X).drop_duplicates().values # remove duplicates in case you use bootstrapping\n",
    "    intdim_k = []\n",
    "    for k in range(k1, k2 + 1): # in order to reduse the noise by eliminating of the nearest neibours \n",
    "        m = intrinsic_dim_sample_wise(X, k).mean()\n",
    "        intdim_k.append(m)\n",
    "    return intdim_k\n",
    " \n",
    "def repeated(func, X, nb_iter=100, random_state=2, mode='bootstrap', **func_kw):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    nb_examples = X.shape[0]\n",
    "    results = []\n",
    " \n",
    "    iters = range(nb_iter) \n",
    "    for i in iters:\n",
    "        if mode == 'bootstrap':# and each point we want to resample with repeating points to reduse the errors \n",
    "            #232 111 133 \n",
    "            Xr = X[rng.randint(0, nb_examples, size=nb_examples)]\n",
    "        elif mode == 'shuffle':\n",
    "            ind = np.arange(nb_examples)\n",
    "            rng.shuffle(ind)\n",
    "            Xr = X[ind]\n",
    "        elif mode == 'same':\n",
    "            Xr = X\n",
    "        else:\n",
    "            raise ValueError('unknown mode : {}'.format(mode))\n",
    "        results.append(func(Xr, **func_kw))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "k1 = 2 # start of interval(included)\n",
    "k2 = 20 # end of interval(included)\n",
    "nb_iter = 10 # more iterations more accuracy\n",
    "\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             X, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping\n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)\n",
    "\n",
    "x = np.arange(k1, k2+1)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(x, np.mean(intdim_k_repeated, axis=0), 'b', label='Mean') # it is the mean walue\n",
    "plt.fill_between(x, \n",
    "                 np.mean(intdim_k_repeated, axis=0) - \n",
    "                 2 * np.std(intdim_k_repeated, axis=0),\n",
    "                 np.mean(intdim_k_repeated, axis=0) + \n",
    "                 2 * np.std(intdim_k_repeated, axis=0),\n",
    "                 alpha=0.3,\n",
    "                 label='CI=95%',\n",
    "                 color='g')\n",
    "plt.xlabel(\"Nearest Neigbours\")\n",
    "plt.ylabel(\"Intrinsic Dimensionality\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans \n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "km = KMeans(3)\n",
    "km.fit(X)\n",
    "\n",
    "plot_3d(X, km.labels_, 'KMeans, n_components = 3')\n",
    "iris_cluster = cluster_scoreing_pivot(X, y, km.labels_, iris_cluster, 'KMeans, 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Density-based Clustering: `DBSCAN`\n",
    "\n",
    "Density-based clustering connects areas of high example density into clusters. This allows for arbitrary-shaped distributions as long as dense areas can be connected. These algorithms have difficulty with data of varying densities and high dimensions. Further, by design, these algorithms do not assign outliers to clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "db = ### YOUR CODE HERE ### # how many components we define here?\n",
    "db.fit(X)\n",
    "\n",
    "y_pred = LabelEncoder().fit_transform(db.labels_)\n",
    "\n",
    "plot_3d(X, y_pred, 'DBSCAN')\n",
    "iris_cluster = cluster_scoreing_pivot(X, y, y_pred, iris_cluster, 'DBSCAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Distribution-based Clustering: `GaussianMixture()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "gm = ### YOUR CODE HERE ### # what if will not define the num. pf components?\n",
    "gm.fit(X)\n",
    "\n",
    "y_pred = gm.predict(X)\n",
    "\n",
    "plot_3d(X, y_pred, 'GaussianMixture, n_components = 3 ')\n",
    "iris_cluster = cluster_scoreing_pivot(X, y, y_pred, iris_cluster, 'GaussianMixture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Hierarchical Clustering: `AgglomerativeClustering()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "ac = ### YOUR CODE HERE ### # what if will define the num. pf components?\n",
    "ac.fit(X)\n",
    "\n",
    "y_pred = LabelEncoder().fit_transform(ac.labels_)\n",
    "\n",
    "plot_3d(X, y_pred, 'AgglomerativeClustering, n_clusters = 3 ')\n",
    "iris_cluster = cluster_scoreing_pivot(X, y, y_pred, iris_cluster, 'AgglomerativeClustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### voilla, let's check the pivot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_cluster.style.background_gradient(axis=1, cmap='PuBu', low=0, high = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recap: dimensionality reduction, anomaly detection and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = fetch_olivetti_faces(shuffle=True, random_state= 42 ).data\n",
    "target = fetch_olivetti_faces(shuffle=True, random_state= 42).target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, stratify=target, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "#data\n",
    "plt.matshow(data[0].reshape(64,64), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Write `torch` compatible dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacesData(torch_data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(FacesData, self).__init__()\n",
    "        self.X = ### YOUR CODE HERE ###                                                          \n",
    "        self.y = y.astype(int)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0] \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].unsqueeze(0), self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = FacesData(X_train, y_train) \n",
    "test_dset = FacesData(X_test, y_test) \n",
    "\n",
    "print(train_dset[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Write Autoencoder model as object of  `torch.nn.Module` class\n",
    "\n",
    "It takes as input encoder and decoder (it will be small neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(MyFirstAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Take a mini-batch as an input, encode it to the latent space and decode back to the original space\n",
    "        x_out = decoder(encoder(x))\n",
    "        :param x: torch.tensor, (MB, x_dim)\n",
    "        :return: torch.tensor, (MB, x_dim)\n",
    "        \"\"\"\n",
    "        z = ### YOUR CODE HERE ### \n",
    "        x_out = ### YOUR CODE HERE ###\n",
    "        return x_out #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define Encoder and Decoder, whey will be symmetrical\n",
    "\n",
    "You should define variable for bottelneck layer - `hid` and for number of neurons in the whole network  - `ss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = 128\n",
    "\n",
    "samples, sample_size = data.shape\n",
    "\n",
    "encoder =  ### YOUR CODE HERE ###\n",
    "\n",
    "decoder =  ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Defining criterion, optimizer, scheduler and data loaders\n",
    "\n",
    "Choose criterion, it will be `nnMSEloss` for now, optimizer and scheduler for optimiser\n",
    "\n",
    "Quiestion: why do we need the scheduler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = MyFirstAE(encoder(60), decoder(60))  \n",
    "criterion = nn.MSELoss() \n",
    "optimizer = ### YOUR CODE HERE ###  \n",
    "scheduler = ### YOUR CODE HERE ###  \n",
    "\n",
    "\n",
    "train_loader = ### YOUR CODE HERE ###\n",
    "val_loader = ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. The main part - write `train` for the network\n",
    "\n",
    "Train will take the batch, send to the devise, encode and decode it and calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, net, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True, save_dir=None):\n",
    "    net.to(device)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        for X, _ in train_loader:\n",
    "            \n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        net.eval()\n",
    "        \n",
    "        for X, _ in val_loader:\n",
    "            X = X.to(device)\n",
    "            out = net(X)\n",
    "            val_loss = criterion(out, X)\n",
    "\n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        freq = max(epochs//20,1)\n",
    "        if verbose and epoch%freq==0:\n",
    "            print('Epoch {}/{} || Loss:  Train {:.4f} | Validation {:.4f}'.format(epoch, epochs, loss.item(), val_loss.item()))\n",
    "            \n",
    "    if save_dir is not None:\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, 'model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Enjoy the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for `MSE` loss lets get < 0.011 on validation, with AE \"bottleneck\" = 50\n",
    "\n",
    "train(300, net, criterion, optimizer, train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=10, nrows=2, figsize=(20, 5))\n",
    "\n",
    "n_pics = 64\n",
    "\n",
    "for i in range(10):\n",
    "    im = train_dset[i][0]\n",
    "    rec = net(im.reshape(1,n_pics**2).to(device))[0]\n",
    "    ax[0, i].imshow(im[0].reshape(n_pics,n_pics), cmap = \"gray\");\n",
    "    ax[1, i].imshow(rec.detach().cpu().reshape(n_pics,n_pics), cmap = \"gray\");\n",
    "    ax[0, i].axis('off')\n",
    "    ax[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import offsetbox\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "def plot_embedding(X, y, images_small=None, title=None):\n",
    "    \"\"\"\n",
    "    Nice plot on first two components of embedding with Offsets.\n",
    "    \n",
    "    \"\"\"\n",
    "    # take only first two columns\n",
    "    X = X[:, :2]\n",
    "    # scaling\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "    plt.figure(figsize=(13,8))\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    for i in range(X.shape[0] - 1):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.RdGy(y[i]),\n",
    "                 fontdict={'weight': 'bold', 'size': 12})\n",
    "        if images_small is not None:\n",
    "            imagebox = OffsetImage(images_small[i], zoom=.4, cmap = 'gray')\n",
    "            ab = AnnotationBbox(imagebox, (X[i, 0], X[i, 1]),\n",
    "                xycoords='data')                                  \n",
    "            ax.add_artist(ab)\n",
    "    \n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  \n",
    "        for i in range(X.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-1:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "X_isomap = Isomap(n_components= 40).fit_transform(data)\n",
    "data_pic = data.reshape((-1, 64, 64))\n",
    "plot_embedding(X_isomap, target, data_pic, \"Isomap. Projection on two components out of 40. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected = net.encoder(torch.Tensor(data)).detach().numpy()\n",
    "data_pic = data.reshape((-1, 64, 64))\n",
    "plot_embedding(X_projected, target, data_pic, \"AE first two variables from latent space projection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's apply clustering methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new pivot\n",
    "face_cluster = pd.DataFrame()\n",
    "\n",
    "# agglomerative clustering on Isomap\n",
    "X = X_isomap\n",
    "y = target\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters = 20)\n",
    "ac.fit(X)\n",
    "\n",
    "y_pred = LabelEncoder().fit_transform(ac.labels_)\n",
    "\n",
    "plot_3d(X, y_pred, 'AgglomerativeClustering, n_clusters = 20 ')\n",
    "face_cluster = cluster_scoreing_pivot(X, y, y_pred, face_cluster , 'AgglomerativeClustering Isomap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_projected\n",
    "y = target\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters = 20)\n",
    "ac.fit(X)\n",
    "\n",
    "y_pred = LabelEncoder().fit_transform(ac.labels_)\n",
    "\n",
    "plot_3d(X, y_pred, 'AgglomerativeClustering, n_clusters = 20 ')\n",
    "face_cluster = cluster_scoreing_pivot(X, y, y_pred, face_cluster , 'AgglomerativeClustering AE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different clustering algoritms to get best score on V-measure on Faces whole sample. \n",
    "\n",
    "What will be more accurate: classification or clustering?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
